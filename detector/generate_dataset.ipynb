{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detector: Dataset Creation\n",
    "\n",
    "This notebook generates the datasets for creating an object detector using TF models. This object detector will detect 4 seperate classes:\n",
    "\n",
    " - 1 - Clouds\n",
    " - 2 - the Sun\n",
    " - 3 - Houses\n",
    " - 4 - Trees\n",
    "\n",
    "Sources:\n",
    "- [1] https://www.oreilly.com/ideas/object-detection-with-tensorflow\n",
    "- [2] https://github.com/tzutalin/labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T18:16:09.044353Z",
     "start_time": "2018-02-02T18:16:09.014243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "# Image sizes\n",
    "MAX_WIDTH = 1280\n",
    "MAX_HEIGHT = 300\n",
    "\n",
    "# Class dictionary\n",
    "CLASS_NAMES = {1:'cloud', 2:'sun', 3:'house', 4:'tree'}\n",
    "\n",
    "# Define paths to sub-folders\n",
    "root_dir = Path.cwd()\n",
    "images_path = root_dir / 'images'\n",
    "labels_path = root_dir / 'labels'\n",
    "train_path = root_dir / 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label images\n",
    "\n",
    "Label the images using LabelImg [2]. This creates an xml file for each image with the bounding boxes and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T18:16:10.086134Z",
     "start_time": "2018-02-02T18:16:10.049468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted xmls to csv file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Convert the XMLs into a single CSV file\n",
    "xml_list = []\n",
    "for xml_path in list(labels_path.glob('*.xml')):\n",
    "    tree = ET.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "    for member in root.findall('object'):\n",
    "        # Unpack each object (BB) from the xml\n",
    "        value = (root.find('filename').text,\n",
    "                 int(root.find('size')[0].text),\n",
    "                 int(root.find('size')[1].text),\n",
    "                 member[0].text,\n",
    "                 int(member[4][0].text),\n",
    "                 int(member[4][1].text),\n",
    "                 int(member[4][2].text),\n",
    "                 int(member[4][3].text))\n",
    "        xml_list.append(value)\n",
    "# Create pandas dataframe from the labels in the XML\n",
    "column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "xml_df.to_csv(train_path / 'labels.csv', index=None)\n",
    "print('Converted xmls to csv file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TFRecords\n",
    "\n",
    "Convert to TFRecords so we can train better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T18:16:11.290294Z",
     "start_time": "2018-02-02T18:16:11.199957Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "def to_tfrecords(image_paths, labels_path, tfrecord_path):\n",
    "    if tfrecord_path.exists():\n",
    "        print('TFRecord already created, delete it before making a new one')\n",
    "        return\n",
    "    writer = tf.python_io.TFRecordWriter(str(tfrecord_path))\n",
    "    # Read labels from csv\n",
    "    label_df = pd.read_csv(str(labels_path))\n",
    "    gb = label_df.groupby('filename')\n",
    "    # Convert each image to a tfrecords example then write\n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            group = gb.get_group(image_path.name)\n",
    "        except KeyError:\n",
    "            print('Could not find labels for %s' % image_path.name)\n",
    "            continue\n",
    "        # Write each serialized example to writer\n",
    "        writer.write(_create_tf_example(image_path, group).SerializeToString())\n",
    "    writer.close()\n",
    "    print('TFRecord created at %s' % str(tfrecord_path))\n",
    "\n",
    "def _create_tf_example(image_path, groups):\n",
    "        image = PIL.Image.open(image_path)\n",
    "        width_raw, height_raw = image.size\n",
    "        # Use thumbnail resizing in order to maintain aspect ratio\n",
    "        image.thumbnail((MAX_WIDTH, MAX_HEIGHT), Image.ANTIALIAS)\n",
    "        width, height = image.size\n",
    "        print('Image has been resized from (%s, %s) to (%s, %s)' % (width_raw, height_raw,\n",
    "                                                                    width, height))\n",
    "        encoded_img = np.array(image).tostring()\n",
    "        # Feature defines each discrete entry in the tfrecords file\n",
    "        filename = image_path.name.encode('utf8')\n",
    "        image_format = b'jpg'\n",
    "        xmins = []\n",
    "        xmaxs = []\n",
    "        ymins = []\n",
    "        ymaxs = []\n",
    "        classes_text = []\n",
    "        classes = []\n",
    "        print('groups: ', groups)\n",
    "        for index, row in groups.iterrows():\n",
    "            xmins.append(row['xmin'] / width)\n",
    "            xmaxs.append(row['xmax'] / width)\n",
    "            ymins.append(row['ymin'] / height)\n",
    "            ymaxs.append(row['ymax'] / height)\n",
    "            classes.append(row['class'])\n",
    "            classes_text.append(CLASS_NAMES[row['class']].encode('utf8'))\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': dataset_util.int64_feature(height),\n",
    "            'image/width': dataset_util.int64_feature(width),\n",
    "            'image/filename': dataset_util.bytes_feature(filename),\n",
    "            'image/source_id': dataset_util.bytes_feature(filename),\n",
    "            'image/encoded': dataset_util.bytes_feature(encoded_img),\n",
    "            'image/format': dataset_util.bytes_feature(image_format),\n",
    "            'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "            'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "            'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "            'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "            'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "            'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        }))\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T18:16:12.501913Z",
     "start_time": "2018-02-02T18:16:12.389014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 images total, split into 6 train and 1 test\n",
      "Could not find labels for 7.jpg\n",
      "Could not find labels for 3.jpg\n",
      "Image has been resized from (275, 183) to (275, 183)\n",
      "groups:    filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "0    1.jpg    640     480      1   114    61   346   152\n",
      "1    1.jpg    640     480      1   391   176   474   221\n",
      "2    1.jpg    640     480      1   485   125   559   160\n",
      "Image has been resized from (960, 540) to (533, 300)\n",
      "groups:    filename  width  height  class  xmin  ymin  xmax  ymax\n",
      "4    2.jpg    640     480      1   525     7   601    50\n",
      "5    2.jpg    640     480      1   302   199   435   254\n",
      "6    2.jpg    640     480      1   142   235   228   289\n",
      "Could not find labels for 4.jpg\n",
      "Could not find labels for 5.jpg\n",
      "TFRecord created at /home/ook/repos/iris_challenge/detector/train/train.tfrecords\n",
      "Could not find labels for 6.jpg\n",
      "TFRecord created at /home/ook/repos/iris_challenge/detector/train/test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "# Filenames (pathlib is so clean :D)\n",
    "train_tfrecord_path = train_path / 'train.tfrecords'\n",
    "test_tfrecord_path = train_path / 'test.tfrecords'\n",
    "labels_path = train_path / 'labels.csv'\n",
    "\n",
    "# Split data into test and train\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "image_paths = list(images_path.glob('*.jpg'))\n",
    "num_images = len(image_paths)\n",
    "num_train = int(TRAIN_TEST_SPLIT * num_images)\n",
    "train_index = np.random.choice(num_images, size=num_train, replace=False)\n",
    "test_index = np.setdiff1d(list(range(num_images)), train_index)\n",
    "train_image_paths = [image_paths[i] for i in train_index]\n",
    "test_image_paths = [image_paths[i] for i in test_index]\n",
    "print('There are %d images total, split into %s train and %s test' % (num_images,\n",
    "                                                                      len(train_image_paths),\n",
    "                                                                      len(test_image_paths)))\n",
    "# Convert list of train and test images into a tfrecord\n",
    "to_tfrecords(train_image_paths, labels_path, train_tfrecord_path)\n",
    "to_tfrecords(test_image_paths, labels_path, test_tfrecord_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vectorize]",
   "language": "python",
   "name": "conda-env-vectorize-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
